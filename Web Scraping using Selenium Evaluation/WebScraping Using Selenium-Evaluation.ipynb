{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c06af7c4-60e2-461f-be23-e8fa660253b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (2024.7.4)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "# First install the selenium library\n",
    "!pip install selenium pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "70b963d0-bdea-4381-95fd-6631e6ceec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required libraries.\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d3694c58-f85c-404a-a465-b9909c0a46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the web driver\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Provide the correct path to your chromedriver\n",
    "service = Service(r\"C:\\Users\\Admin\\Downloads\\chromedriver-win32\\chromedriver.exe\")\n",
    "\n",
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3a68069-6182-4e3c-b900-1f580e937a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver_manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver_manager) (2.32.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver_manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver_manager) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2024.7.4)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: webdriver_manager\n",
      "Successfully installed webdriver_manager-4.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "664173fa-eb4f-4931-bba9-a7815cd79455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.25.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (2024.7.4)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver-manager) (23.2)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb3ae70a-4be2-4b54-85d4-32b7431d8fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.25.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (2024.7.4)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver-manager) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium pandas webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1121b-c6fd-465b-b933-e4b059fa463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q1: In this question you have to scrape data using the filters available on the webpage You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the web page https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "db189d3d-12ed-4f63-bdd4-f3e703672ee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error locating search field: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x004D83E3+25571]\n",
      "\t(No symbol) [0x0046A684]\n",
      "\t(No symbol) [0x00362113]\n",
      "\t(No symbol) [0x003A6FB2]\n",
      "\t(No symbol) [0x003A71FB]\n",
      "\t(No symbol) [0x003E7822]\n",
      "\t(No symbol) [0x003CAC54]\n",
      "\t(No symbol) [0x003E5349]\n",
      "\t(No symbol) [0x003CA9A6]\n",
      "\t(No symbol) [0x0039BAB6]\n",
      "\t(No symbol) [0x0039C50D]\n",
      "\tGetHandleVerifier [0x007AC4A3+2991267]\n",
      "\tGetHandleVerifier [0x007FD2C9+3322569]\n",
      "\tGetHandleVerifier [0x005684D2+615634]\n",
      "\tGetHandleVerifier [0x0056FBFC+646140]\n",
      "\t(No symbol) [0x0047327D]\n",
      "\t(No symbol) [0x00470188]\n",
      "\t(No symbol) [0x00470325]\n",
      "\t(No symbol) [0x00462826]\n",
      "\tBaseThreadInitThunk [0x76BBFCC9+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77DE80CE+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77DE809E+238]\n",
      "\n",
      "Error clicking search button: HTTPConnectionPool(host='localhost', port=53103): Max retries exceeded with url: /session/ba134ad8ed334ff60859ef0cc52fd09f/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001B8F1FCF3B0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "Error applying location filter: HTTPConnectionPool(host='localhost', port=53103): Max retries exceeded with url: /session/ba134ad8ed334ff60859ef0cc52fd09f/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001B8F1F6D070>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "Error applying salary filter: HTTPConnectionPool(host='localhost', port=53103): Max retries exceeded with url: /session/ba134ad8ed334ff60859ef0cc52fd09f/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001B8F1FCE180>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "Error scraping job listings: HTTPConnectionPool(host='localhost', port=53103): Max retries exceeded with url: /session/ba134ad8ed334ff60859ef0cc52fd09f/elements (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001B8F1F6EEA0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up the Chrome WebDriver with Service\n",
    "service = Service(\"C:\\\\Users\\\\Admin\\\\Downloads\\\\chromedriver-win32\\\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Step 1: Open the Naukri.com webpage\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "# Maximize window to ensure all elements are visible\n",
    "driver.maximize_window()\n",
    "\n",
    "# Step 2: Wait for the search bar to be present and enter \"Data Scientist\"\n",
    "wait = WebDriverWait(driver, 20)  # Increased wait time to 20 seconds\n",
    "try:\n",
    "    search_field = wait.until(EC.visibility_of_element_located((By.XPATH, \"//input[contains(@class, 'sugInp')]\")))\n",
    "    search_field.send_keys(\"Data Scientist\")\n",
    "    print(\"Search field located and query entered.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error locating search field: {e}\")\n",
    "    driver.quit()\n",
    "\n",
    "# Step 3: Click the search button\n",
    "try:\n",
    "    search_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(text(),'Search')]\")))\n",
    "    search_button.click()\n",
    "    print(\"Search button clicked.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error clicking search button: {e}\")\n",
    "    driver.quit()\n",
    "\n",
    "# Step 4: Apply the location filter for \"Delhi/NCR\"\n",
    "time.sleep(5)  # Allow the page to load before applying filters\n",
    "try:\n",
    "    location_filter = wait.until(EC.element_to_be_clickable((By.XPATH, \"//label[contains(text(),'Delhi / NCR')]\")))\n",
    "    location_filter.click()\n",
    "    print(\"Location filter applied.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error applying location filter: {e}\")\n",
    "    driver.quit()\n",
    "\n",
    "# Step 5: Apply the salary filter for \"3-6 Lakhs\"\n",
    "time.sleep(2)  # Wait for the location filter to apply\n",
    "try:\n",
    "    salary_filter = wait.until(EC.element_to_be_clickable((By.XPATH, \"//label[contains(text(),'3-6 Lakhs')]\")))\n",
    "    salary_filter.click()\n",
    "    print(\"Salary filter applied.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error applying salary filter: {e}\")\n",
    "    driver.quit()\n",
    "\n",
    "# Step 6: Scrape the data for the first 10 job results\n",
    "time.sleep(5)  # Wait for the filtered job results to load\n",
    "try:\n",
    "    jobs = driver.find_elements(By.XPATH, \"//article[@class='jobTuple bgWhite br4 mb-8']\")[:10]\n",
    "    job_data = []\n",
    "    \n",
    "    for job in jobs:\n",
    "        try:\n",
    "            # Scrape job title\n",
    "            title = job.find_element(By.XPATH, \".//a[@class='title fw500 ellipsis']\").text\n",
    "\n",
    "            # Scrape job location\n",
    "            location = job.find_element(By.XPATH, \".//li[contains(@class, 'placeHolderLi location')]\").text\n",
    "\n",
    "            # Scrape company name\n",
    "            company = job.find_element(By.XPATH, \".//a[contains(@class, 'subTitle ellipsis fleft')]\").text\n",
    "\n",
    "            # Scrape experience required\n",
    "            experience = job.find_element(By.XPATH, \".//li[contains(@class, 'placeHolderLi experience')]\").text\n",
    "\n",
    "            # Add to job_data list\n",
    "            job_data.append({\n",
    "                'Job Title': title,\n",
    "                'Location': location,\n",
    "                'Company Name': company,\n",
    "                'Experience Required': experience\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping job data: {e}\")\n",
    "    \n",
    "    print(\"Job data scraped successfully.\")\n",
    "\n",
    "    # Step 7: Create a pandas dataframe from the scraped data\n",
    "    df = pd.DataFrame(job_data)\n",
    "\n",
    "    # Display the dataframe\n",
    "    print(df)\n",
    "\n",
    "    # Save to a CSV file (optional)\n",
    "    df.to_csv('naukri_jobs.csv', index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error scraping job listings: {e}\")\n",
    "    driver.quit()\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f843079-278f-4c11-85a6-6a631c8c90af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Q.2. Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the\\njob-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\\nThis task will be done in following steps:\\n1. First get the webpage https://www.shine.com/\\n2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\\n3. Then click the searchbutton.\\n4. Then scrape the data for the first 10 jobs results you get.\\n5. Finally create a dataframe of the scraped data. '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Q.2. Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the\n",
    "job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eca09ae8-bd6d-4bfa-a014-5eac3286664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x004D83E3+25571]\n",
      "\t(No symbol) [0x0046A684]\n",
      "\t(No symbol) [0x00362113]\n",
      "\t(No symbol) [0x003A6FB2]\n",
      "\t(No symbol) [0x003A71FB]\n",
      "\t(No symbol) [0x003E7822]\n",
      "\t(No symbol) [0x003CAC54]\n",
      "\t(No symbol) [0x003E5349]\n",
      "\t(No symbol) [0x003CA9A6]\n",
      "\t(No symbol) [0x0039BAB6]\n",
      "\t(No symbol) [0x0039C50D]\n",
      "\tGetHandleVerifier [0x007AC4A3+2991267]\n",
      "\tGetHandleVerifier [0x007FD2C9+3322569]\n",
      "\tGetHandleVerifier [0x005684D2+615634]\n",
      "\tGetHandleVerifier [0x0056FBFC+646140]\n",
      "\t(No symbol) [0x0047327D]\n",
      "\t(No symbol) [0x00470188]\n",
      "\t(No symbol) [0x00470325]\n",
      "\t(No symbol) [0x00462826]\n",
      "\tBaseThreadInitThunk [0x76BBFCC9+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77DE80CE+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77DE809E+238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set up the Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=Service('C:\\\\Users\\\\Admin\\\\Downloads\\\\chromedriver-win32\\\\chromedriver.exe'))\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "\n",
    "try:\n",
    "    # Step 2: Enter \"Data Scientist\" in the job title field\n",
    "    wait = WebDriverWait(driver, 15)  # Increased wait time\n",
    "    job_title_field = wait.until(EC.presence_of_element_located((By.XPATH, \"//input[@id='sh-job-title']\")))\n",
    "    job_title_field.send_keys(\"Data Scientist\")\n",
    "\n",
    "    # Enter \"Bangalore\" in the location field\n",
    "    location_field = driver.find_element(By.XPATH, \"//input[@id='sh-location']\")    \n",
    "    location_field.send_keys(\"Bangalore\")\n",
    "\n",
    "    # Click the search button\n",
    "    search_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@class='search-btn']\")))\n",
    "    search_button.click()\n",
    "\n",
    "    # Step 4: Scrape the data for the first 10 job results\n",
    "    wait.until(EC.presence_of_all_elements_located((By.XPATH, \"//div[@class='job-card']\")))\n",
    "    job_cards = driver.find_elements(By.XPATH, \"//div[@class='job-card']\")[:10]\n",
    "\n",
    "    # Collect data\n",
    "    job_data = []\n",
    "    for job in job_cards:\n",
    "        try:\n",
    "            title = job.find_element(By.XPATH, \".//h2[@class='job-title']\").text\n",
    "            location = job.find_element(By.XPATH, \".//span[@class='job-location']\").text\n",
    "            company = job.find_element(By.XPATH, \".//a[@class='company-name']\").text\n",
    "            experience = job.find_element(By.XPATH, \".//span[@class='experience']\").text\n",
    "            job_data.append({\n",
    "                \"Job Title\": title,\n",
    "                \"Location\": location,\n",
    "                \"Company Name\": company,\n",
    "                \"Experience Required\": experience\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping job data: {e}\")\n",
    "\n",
    "    # Step 5: Create a DataFrame\n",
    "    df = pd.DataFrame(job_data)\n",
    "    print(df)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cb56c6b-a001-4bbc-a293-0bef43e7c380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Q3: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\\nhttps://www.flipkart.com/apple-iphone-11-black-64-gb/product-\\nreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\\nAs shown in the above page you have to scrape the tick marked attributes. These are:\\n1. Rating\\n2. Review summary\\n3. Full review\\n4. You have to scrape this data for first 100reviews.\\nNote: All the stepsrequired during scraping should be done through code only and not manually.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Q3: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/product-\n",
    "reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100reviews.\n",
    "Note: All the stepsrequired during scraping should be done through code only and not manually.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "86e0b1dd-aaff-433e-b6a9-9b0dd16289cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=129.0.6668.59)\nStacktrace:\n\tGetHandleVerifier [0x004D83E3+25571]\n\t(No symbol) [0x0046A684]\n\t(No symbol) [0x00362113]\n\t(No symbol) [0x0033E23B]\n\t(No symbol) [0x003D17FF]\n\t(No symbol) [0x003E4C99]\n\t(No symbol) [0x003CA9A6]\n\t(No symbol) [0x0039BAB6]\n\t(No symbol) [0x0039C50D]\n\tGetHandleVerifier [0x007AC4A3+2991267]\n\tGetHandleVerifier [0x007FD2C9+3322569]\n\tGetHandleVerifier [0x005684D2+615634]\n\tGetHandleVerifier [0x0056FBFC+646140]\n\t(No symbol) [0x0047327D]\n\t(No symbol) [0x00470188]\n\t(No symbol) [0x00470325]\n\t(No symbol) [0x00462826]\n\tBaseThreadInitThunk [0x76BBFCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x77DE80CE+286]\n\tRtlGetAppContainerNamedObjectPath [0x77DE809E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Get all review elements\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m review_elements \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//div[contains(@class, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m review_elements:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(reviews) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m num_reviews_to_scrape:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:778\u001b[0m, in \u001b[0;36mWebDriver.find_elements\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    774\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;66;03m# Return empty list if driver returns null\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;66;03m# See https://github.com/SeleniumHQ/selenium/issues/4555\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mFIND_ELEMENTS, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m\"\u001b[39m: by, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=129.0.6668.59)\nStacktrace:\n\tGetHandleVerifier [0x004D83E3+25571]\n\t(No symbol) [0x0046A684]\n\t(No symbol) [0x00362113]\n\t(No symbol) [0x0033E23B]\n\t(No symbol) [0x003D17FF]\n\t(No symbol) [0x003E4C99]\n\t(No symbol) [0x003CA9A6]\n\t(No symbol) [0x0039BAB6]\n\t(No symbol) [0x0039C50D]\n\tGetHandleVerifier [0x007AC4A3+2991267]\n\tGetHandleVerifier [0x007FD2C9+3322569]\n\tGetHandleVerifier [0x005684D2+615634]\n\tGetHandleVerifier [0x0056FBFC+646140]\n\t(No symbol) [0x0047327D]\n\t(No symbol) [0x00470188]\n\t(No symbol) [0x00470325]\n\t(No symbol) [0x00462826]\n\tBaseThreadInitThunk [0x76BBFCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x77DE80CE+286]\n\tRtlGetAppContainerNamedObjectPath [0x77DE809E+238]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Set up the Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=Service('C:\\\\Users\\\\Admin\\\\Downloads\\\\chromedriver-win32\\\\chromedriver.exe'))\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\")\n",
    "\n",
    "# Prepare to scrape reviews\n",
    "reviews = []\n",
    "num_reviews_to_scrape = 100\n",
    "\n",
    "while len(reviews) < num_reviews_to_scrape:\n",
    "    # Wait for the reviews to load\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Get all review elements\n",
    "    review_elements = driver.find_elements(By.XPATH, \"//div[contains(@class, 'review')]\")\n",
    "    \n",
    "    for review in review_elements:\n",
    "        if len(reviews) >= num_reviews_to_scrape:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Extract Rating\n",
    "            rating = review.find_element(By.XPATH, \".//div[contains(@class, '_3LWZlK')]\").text\n",
    "            \n",
    "            # Extract Review Summary\n",
    "            review_summary = review.find_element(By.XPATH, \".//p[contains(@class, '_2-N8zT')]\").text\n",
    "            \n",
    "            # Extract Full Review\n",
    "            full_review = review.find_element(By.XPATH, \".//div[contains(@class, 't-ZTKy')]//div\").text\n",
    "            \n",
    "            reviews.append({\n",
    "                \"Rating\": rating,\n",
    "                \"Review Summary\": review_summary,\n",
    "                \"Full Review\": full_review\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting review: {e}\")\n",
    "\n",
    "    # Scroll down to load more reviews\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    # Optionally, wait for a moment to ensure new reviews load\n",
    "    time.sleep(2)\n",
    "\n",
    "# Create a DataFrame from the reviews\n",
    "df = pd.DataFrame(reviews)\n",
    "\n",
    "# After scraping is complete\n",
    "print(reviews_df)\n",
    "\n",
    "# Optionally save to a CSV file\n",
    "reviews_df.to_csv('iphone11_reviews.csv', index=False)\n",
    "\n",
    "# Clean up\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b2384d-c553-4562-920d-7325a1e84be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q4: Scrape data forfirst 100 sneakers you find whenyouvisitflipkart.com and search for “sneakers” in the search\n",
    "field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the above attributes.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eeff9190-943d-4514-884d-984d01741b02",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=129.0.6668.59)\nStacktrace:\n\tGetHandleVerifier [0x004D83E3+25571]\n\t(No symbol) [0x0046A684]\n\t(No symbol) [0x00362113]\n\t(No symbol) [0x0033E23B]\n\t(No symbol) [0x003D17FF]\n\t(No symbol) [0x003E4C99]\n\t(No symbol) [0x003CA9A6]\n\t(No symbol) [0x0039BAB6]\n\t(No symbol) [0x0039C50D]\n\tGetHandleVerifier [0x007AC4A3+2991267]\n\tGetHandleVerifier [0x007FD2C9+3322569]\n\tGetHandleVerifier [0x005684D2+615634]\n\tGetHandleVerifier [0x0056FBFC+646140]\n\t(No symbol) [0x0047327D]\n\t(No symbol) [0x00470188]\n\t(No symbol) [0x00470325]\n\t(No symbol) [0x00462826]\n\tBaseThreadInitThunk [0x76BBFCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x77DE80CE+286]\n\tRtlGetAppContainerNamedObjectPath [0x77DE809E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m num_sneakers_to_scrape \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sneakers) \u001b[38;5;241m<\u001b[39m num_sneakers_to_scrape:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Get all sneaker elements\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     sneaker_elements \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//div[contains(@class, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_1AtVbE\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)]//div[contains(@class, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_2kHMtA\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sneaker \u001b[38;5;129;01min\u001b[39;00m sneaker_elements:\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sneakers) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m num_sneakers_to_scrape:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:778\u001b[0m, in \u001b[0;36mWebDriver.find_elements\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    774\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;66;03m# Return empty list if driver returns null\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;66;03m# See https://github.com/SeleniumHQ/selenium/issues/4555\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mFIND_ELEMENTS, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m\"\u001b[39m: by, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=129.0.6668.59)\nStacktrace:\n\tGetHandleVerifier [0x004D83E3+25571]\n\t(No symbol) [0x0046A684]\n\t(No symbol) [0x00362113]\n\t(No symbol) [0x0033E23B]\n\t(No symbol) [0x003D17FF]\n\t(No symbol) [0x003E4C99]\n\t(No symbol) [0x003CA9A6]\n\t(No symbol) [0x0039BAB6]\n\t(No symbol) [0x0039C50D]\n\tGetHandleVerifier [0x007AC4A3+2991267]\n\tGetHandleVerifier [0x007FD2C9+3322569]\n\tGetHandleVerifier [0x005684D2+615634]\n\tGetHandleVerifier [0x0056FBFC+646140]\n\t(No symbol) [0x0047327D]\n\t(No symbol) [0x00470188]\n\t(No symbol) [0x00470325]\n\t(No symbol) [0x00462826]\n\tBaseThreadInitThunk [0x76BBFCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x77DE80CE+286]\n\tRtlGetAppContainerNamedObjectPath [0x77DE809E+238]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Set up the Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=Service('C:\\\\Users\\\\Admin\\\\Downloads\\\\chromedriver-win32\\\\chromedriver.exe'))\n",
    "\n",
    "# Navigate to Flipkart and search for \"sneakers\"\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "# Search for \"sneakers\"\n",
    "search_box = driver.find_element(By.NAME, \"q\")\n",
    "search_box.send_keys(\"sneakers\")\n",
    "search_box.submit()\n",
    "time.sleep(2)  # Wait for the search results to load\n",
    "\n",
    "# Prepare to scrape sneakers data\n",
    "sneakers = []\n",
    "num_sneakers_to_scrape = 100\n",
    "\n",
    "while len(sneakers) < num_sneakers_to_scrape:\n",
    "    # Get all sneaker elements\n",
    "    sneaker_elements = driver.find_elements(By.XPATH, \"//div[contains(@class, '_1AtVbE')]//div[contains(@class, '_2kHMtA')]\")\n",
    "\n",
    "    for sneaker in sneaker_elements:\n",
    "        if len(sneakers) >= num_sneakers_to_scrape:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Extract Brand\n",
    "            brand = sneaker.find_element(By.XPATH, \".//div[contains(@class, '_2WkVRV')]\").text\n",
    "            \n",
    "            # Extract Product Description\n",
    "            product_description = sneaker.find_element(By.XPATH, \".//a[contains(@class, 'IRpwTa')]\").text\n",
    "            \n",
    "            # Extract Price\n",
    "            price = sneaker.find_element(By.XPATH, \".//div[contains(@class, '_30jeq3')]\").text\n",
    "            \n",
    "            sneakers.append({\n",
    "                \"Brand\": brand,\n",
    "                \"Product Description\": product_description,\n",
    "                \"Price\": price\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting sneaker data: {e}\")\n",
    "\n",
    "    # Scroll down to load more sneakers\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # Wait for new sneakers to load\n",
    "\n",
    "# Create a DataFrame from the sneakers data\n",
    "df = pd.DataFrame(sneakers)\n",
    "\n",
    "# After scraping is complete\n",
    "print(df)\n",
    "\n",
    "# Optionally save to a CSV file\n",
    "df.to_csv('sneakers_data.csv', index=False)\n",
    "\n",
    "# Clean up\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc21204-74b8-4562-853e-aa19bf7a08cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q5: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU\n",
    "Type filter to “Intel Core i7” as shown in the below image:\n",
    "Aftersetting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "827c5c4b-ead1-4ec7-84b8-915597ac6c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title     Rating     Price\n",
      "0  HP Laptop 15s, 12th Gen Intel Core i7-1255U, 1...               59,990\n",
      "1  HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...               76,990\n",
      "2  Dell [Smartchoice] Inspiron 5430 Thin & Light ...               75,490\n",
      "3  Acer Aspire Lite 12th Gen Intel Core i7-1255U ...               49,990\n",
      "4  Acer ALG 13th Gen Intel Core i7 Gaming Laptop ...  No rating    71,990\n",
      "5  (Refurbished) Dell Latitude 7480 14in FHD Lapt...               27,531\n",
      "6  ASUS Vivobook 15, 15.6\" (39.62cm) FHD, Intel C...               61,990\n",
      "7  Acer Aspire 3 Intel Core i7 12th Gen 1255U - (...  No rating    57,990\n",
      "8  MSI Thin 15, Intel 13th Gen. Core i7-13620H, 4...               69,990\n",
      "9  ASUS TUF Gaming F15, 15.6\" (39.62cm) FHD 144Hz...             1,08,048\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Set up the Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=Service('C:\\\\Users\\\\Admin\\\\Downloads\\\\chromedriver-win32\\\\chromedriver.exe'))\n",
    "\n",
    "# Step 1: Navigate to Amazon India\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "# Step 2: Enter \"Laptop\" in the search field and click the search icon\n",
    "search_box = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "search_box.send_keys(\"Laptop\")\n",
    "search_box.submit()\n",
    "time.sleep(2)  # Wait for the search results to load\n",
    "\n",
    "# Step 3: Apply the \"Intel Core i7\" filter for CPU Type\n",
    "try:\n",
    "    # Locate and click on the \"Intel Core i7\" filter checkbox\n",
    "    intel_core_i7_filter = driver.find_element(By.XPATH, \"//li[@aria-label='Intel Core i7']//span[@class='a-list-item']\")\n",
    "    intel_core_i7_filter.click()\n",
    "    time.sleep(2)  # Wait for the filter to apply\n",
    "except Exception as e:\n",
    "    print(f\"Error applying Intel Core i7 filter: {e}\")\n",
    "\n",
    "# Step 4: Scrape data for the first 10 laptops\n",
    "laptops = []\n",
    "num_laptops_to_scrape = 10\n",
    "\n",
    "# Loop to scrape the first 10 laptop listings\n",
    "while len(laptops) < num_laptops_to_scrape:\n",
    "    laptop_elements = driver.find_elements(By.XPATH, \"//div[contains(@class, 's-main-slot')]//div[@data-component-type='s-search-result']\")\n",
    "\n",
    "    for laptop in laptop_elements:\n",
    "        if len(laptops) >= num_laptops_to_scrape:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Extract Title\n",
    "            title = laptop.find_element(By.XPATH, \".//span[contains(@class, 'a-size-medium')]\").text\n",
    "            \n",
    "            # Extract Ratings (Note: Not all products have ratings, so we handle exceptions)\n",
    "            try:\n",
    "                rating = laptop.find_element(By.XPATH, \".//span[contains(@class, 'a-icon-alt')]\").text\n",
    "            except:\n",
    "                rating = \"No rating\"\n",
    "            \n",
    "            # Extract Price\n",
    "            try:\n",
    "                price = laptop.find_element(By.XPATH, \".//span[contains(@class, 'a-price-whole')]\").text\n",
    "            except:\n",
    "                price = \"Price not available\"\n",
    "\n",
    "            laptops.append({\n",
    "                \"Title\": title,\n",
    "                \"Rating\": rating,\n",
    "                \"Price\": price\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting laptop data: {e}\")\n",
    "\n",
    "    # Scroll down and wait for more laptops to load (if necessary)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # Wait for new laptops to load\n",
    "\n",
    "# Create a DataFrame from the laptops data\n",
    "df = pd.DataFrame(laptops)\n",
    "\n",
    "# Step 5: Print the DataFrame and optionally save it to a CSV file\n",
    "print(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('laptops_data.csv', index=False)\n",
    "\n",
    "# Clean up by closing the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c62808-d438-4fd9-977c-4b995a1564d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q6: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on TopQuote\n",
    "3. Than scrap a)Quote b) Author c) Type Of Quotes\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9591efa-a7c2-48b3-8b05-9be74ae306a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages available.\n",
      "                                                 Quote              Author  \\\n",
      "0    The essence of strategy is choosing what not t...      Michael Porter   \n",
      "1    One cannot and must not try to erase the past ...          Golda Meir   \n",
      "2    Patriotism means to stand by the country. It d...  Theodore Roosevelt   \n",
      "3    Death is something inevitable. When a man has ...      Nelson Mandela   \n",
      "4    You have to love a nation that celebrates its ...        Erma Bombeck   \n",
      "..                                                 ...                 ...   \n",
      "995  Regret for the things we did can be tempered b...    Sydney J. Harris   \n",
      "996  America... just a nation of two hundred millio...  Hunter S. Thompson   \n",
      "997  For every disciplined effort there is a multip...            Jim Rohn   \n",
      "998  The spiritual journey is individual, highly pe...            Ram Dass   \n",
      "999  The mind is not a vessel to be filled but a fi...            Plutarch   \n",
      "\n",
      "                                Type of Quote  \n",
      "0    Essence, Deep Thought, Transcendentalism  \n",
      "1                   Inspiration, Past, Trying  \n",
      "2                         Country, Peace, War  \n",
      "3          Inspirational, Motivational, Death  \n",
      "4                4th Of July, Food, Patriotic  \n",
      "..                                        ...  \n",
      "995         Love, Inspirational, Motivational  \n",
      "996                    Gun, Two, Qualms About  \n",
      "997     Inspirational, Greatness, Best Effort  \n",
      "998                    Spiritual, Truth, Yoga  \n",
      "999      Inspirational, Leadership, Education  \n",
      "\n",
      "[1000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Set up Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Step 1: Open the AZ Quotes website\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "# Step 2: Click on the \"Top Quotes\" link using XPath instead of LINK_TEXT\n",
    "try:\n",
    "    top_quotes_link = driver.find_element(By.XPATH, \"//a[contains(text(), 'Top Quotes')]\")\n",
    "    top_quotes_link.click()\n",
    "    time.sleep(2)  # Wait for the page to load\n",
    "except Exception as e:\n",
    "    print(f\"Error finding 'Top Quotes' link: {e}\")\n",
    "\n",
    "# Step 3: Continue with scraping \n",
    "quotes = []\n",
    "num_quotes_to_scrape = 1000\n",
    "\n",
    "while len(quotes) < num_quotes_to_scrape:\n",
    "    # Get all the quote elements on the current page\n",
    "    quote_elements = driver.find_elements(By.XPATH, \"//div[@class='wrap-block']\")\n",
    "\n",
    "    for quote_element in quote_elements:\n",
    "        if len(quotes) >= num_quotes_to_scrape:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Extract Quote Text\n",
    "            quote_text = quote_element.find_element(By.XPATH, \".//a[contains(@class, 'title')]\").text\n",
    "            \n",
    "            # Extract Author\n",
    "            author = quote_element.find_element(By.XPATH, \".//div[contains(@class, 'author')]\").text\n",
    "            \n",
    "            # Extract Type of Quote (from the 'tags' section)\n",
    "            try:\n",
    "                quote_type = quote_element.find_element(By.XPATH, \".//div[contains(@class, 'tags')]\").text\n",
    "            except:\n",
    "                quote_type = \"No type available\"  # Default if no type is available\n",
    "\n",
    "            # Append the extracted data to the list\n",
    "            quotes.append({\n",
    "                \"Quote\": quote_text,\n",
    "                \"Author\": author,\n",
    "                \"Type of Quote\": quote_type\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting quote data: {e}\")\n",
    "    \n",
    "    # Step 4: Move to the next page if more quotes are needed\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, \"//li[@class='next']/a\")\n",
    "        next_button.click()\n",
    "        time.sleep(2)  # Wait for the next page to load\n",
    "    except:\n",
    "        print(\"No more pages available.\")\n",
    "        break\n",
    "\n",
    "# Step 5: Create a DataFrame from the scraped quotes\n",
    "df = pd.DataFrame(quotes)\n",
    "\n",
    "# Print the DataFrame to verify the result\n",
    "print(df)\n",
    "\n",
    "# Optionally, save the scraped data to a CSV file\n",
    "df.to_csv('top_1000_quotes.csv', index=False)\n",
    "\n",
    "# Clean up and close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16116f-b695-43ac-b044-c8433aa90cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Q7: Write a python program to display list of respected former Prime Ministers of India (i.e. Name,\n",
    "Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/general-knowledge/list-of-\n",
    "all-prime-ministers-of-india-1473165149-1\n",
    "\n",
    "scrap the mentioned data and make the DataFrame\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "830eb002-5224-4e95-8fea-4af7b3f7a08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Name                Born-Dead      Term of Office           Remarks\n",
      "0     1         Jawaharlal Nehru  57 years, 274 days     15 August1947\n",
      "1     2         Gulzarilal Nanda  65 years, 328 days       27 May 1964\n",
      "2     3      Lal Bahadur Shastri  62 years, 250 days       9 June 1964\n",
      "3     4         Gulzarilal Nanda  67 years, 191 days   11 January 1966\n",
      "4     4            Indira Gandhi   48 years, 66 days   24 January 1966\n",
      "5     5            Morarji Desai   81 years, 24 days     24 March 1977\n",
      "6     6             Charan Singh  76 years, 217 days      28 July 1979\n",
      "7     7            Indira Gandhi   62 years, 56 days   14 January 1980\n",
      "8     8             Rajiv Gandhi   40 years, 72 days   31 October 1984\n",
      "9     9  Vishwanath Pratap Singh  58 years, 160 days   2 December 1989\n",
      "10   10          Chandra Shekhar  63 years, 207 days  10 November 1990\n",
      "11   11      P. V. Narasimha Rao  69 years, 358 days      21 June 1991\n",
      "12   12     Atal Bihari Vajpayee  71 years, 143 days       16 May 1996\n",
      "13   13         H. D. Deve Gowda   63 years, 14 days       1 June 1996\n",
      "14   14       Inder Kumar Gujral  77 years, 138 days     21 April 1997\n",
      "15   15     Atal Bihari Vajpayee   73 years, 84 days     19 March 1998\n",
      "16   16           Manmohan Singh  71 years, 239 days       22 May 2004\n",
      "17   17            Narendra Modi  63 years, 251 days       26 May 2014\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Set up the WebDriver (make sure the path to the chromedriver is correct)\n",
    "service = Service('C:\\\\Users\\\\Admin\\\\Downloads\\\\chromedriver-win32\\\\chromedriver.exe')  # Update the path if necessary\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# URL of the page containing the list of Prime Ministers\n",
    "url = \"https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)  # Adjust the sleep time if necessary\n",
    "\n",
    "# Find the table containing the data\n",
    "table = driver.find_element(By.TAG_NAME, 'table')\n",
    "\n",
    "# Initialize lists to store the data\n",
    "names = []\n",
    "born_dead = []\n",
    "terms_of_office = []\n",
    "remarks = []\n",
    "\n",
    "# Iterate through the rows of the table (skipping the header)\n",
    "rows = table.find_elements(By.TAG_NAME, 'tr')[1:]  # Skip the header\n",
    "for row in rows:\n",
    "    cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "    if len(cols) >= 4:  # Ensure there are enough columns\n",
    "        names.append(cols[0].text.strip())\n",
    "        born_dead.append(cols[1].text.strip())\n",
    "        terms_of_office.append(cols[2].text.strip())\n",
    "        remarks.append(cols[3].text.strip())\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'Name': names,\n",
    "    'Born-Dead': born_dead,\n",
    "    'Term of Office': terms_of_office,\n",
    "    'Remarks': remarks\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0346f64-8f23-4d76-b0b6-a196eee51f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Q8: Write a python program to display list of 50 Most expensive cars in the world\n",
    "(i.e. Car name and Price) from https://www.motor1.com/\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap thementioned data and make the dataframe.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e51c5b60-b9f8-4437-9a00-18065a0a1197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x004D83E3+25571]\n",
      "\t(No symbol) [0x0046A684]\n",
      "\t(No symbol) [0x00362113]\n",
      "\t(No symbol) [0x003A6FB2]\n",
      "\t(No symbol) [0x003A71FB]\n",
      "\t(No symbol) [0x003E7822]\n",
      "\t(No symbol) [0x003CAC54]\n",
      "\t(No symbol) [0x003E5349]\n",
      "\t(No symbol) [0x003CA9A6]\n",
      "\t(No symbol) [0x0039BAB6]\n",
      "\t(No symbol) [0x0039C50D]\n",
      "\tGetHandleVerifier [0x007AC4A3+2991267]\n",
      "\tGetHandleVerifier [0x007FD2C9+3322569]\n",
      "\tGetHandleVerifier [0x005684D2+615634]\n",
      "\tGetHandleVerifier [0x0056FBFC+646140]\n",
      "\t(No symbol) [0x0047327D]\n",
      "\t(No symbol) [0x00470188]\n",
      "\t(No symbol) [0x00470325]\n",
      "\t(No symbol) [0x00462826]\n",
      "\tBaseThreadInitThunk [0x76BBFCC9+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77DE80CE+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77DE809E+238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Set up the WebDriver (update the path to the ChromeDriver if necessary)\n",
    "service = Service('C:\\\\Users\\\\Admin\\\\Downloads\\\\chromedriver-win32\\\\chromedriver.exe')  # Update the path to your ChromeDriver\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "try:\n",
    "    # Step 1: Get the webpage\n",
    "    url = \"https://www.motor1.com/\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # Step 2: Type in the search bar\n",
    "    search_bar = driver.find_element(By.NAME, 'q')  # Locate the search bar\n",
    "    search_bar.send_keys('50 most expensive cars')  # Type the query\n",
    "    search_bar.submit()  # Submit the search\n",
    "\n",
    "    # Step 3: Wait for the results and click on the article for \"50 most expensive cars\"\n",
    "    first_result = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//a[contains(@href, '50-most-expensive-cars')]\"))\n",
    "    )\n",
    "    first_result.click()\n",
    "\n",
    "    # Wait for the article to load\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//h1[contains(text(), '50 Most Expensive Cars')]\"))\n",
    "    )\n",
    "\n",
    "    # Step 4: Scrape the data\n",
    "    car_names = []\n",
    "    car_prices = []\n",
    "\n",
    "    # Find the elements containing the car names and prices\n",
    "    car_elements = driver.find_elements(By.XPATH, \"//div[@class='article-item__title']\")\n",
    "    price_elements = driver.find_elements(By.XPATH, \"//div[@class='article-item__price']\")\n",
    "\n",
    "    # Extract text from the elements\n",
    "    for car in car_elements:\n",
    "        car_names.append(car.text.strip())\n",
    "\n",
    "    for price in price_elements:\n",
    "        car_prices.append(price.text.strip())\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data = {\n",
    "        'Car Name': car_names,\n",
    "        'Price': car_prices\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26bd6e4-b023-4936-b0de-beeb833363dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
